{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T00:01:31.347035Z",
     "start_time": "2024-12-17T00:01:28.820811Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import sqlite3\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.document_loaders import UnstructuredCSVLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T00:01:32.180107Z",
     "start_time": "2024-12-17T00:01:32.162472Z"
    }
   },
   "id": "446e0d0ff35cd44a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def answer_question_with_glossary_and_data(question):\n",
    "\n",
    "    def load_info(file_paths):\n",
    "        combined_content = \"\"\n",
    "\n",
    "        for file in file_paths:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                combined_content += f.read() + \"\\n\"\n",
    "\n",
    "        return combined_content\n",
    "\n",
    "    # Combina arquivos com informações adicionais\n",
    "    combined_info = load_info(file_paths=[r'C:\\Users\\Wellbe\\DataspellProjects\\dsProject\\football_api\\files\\infos\\glossary.txt'])\n",
    "\n",
    "\n",
    "    loader = UnstructuredCSVLoader(r'C:\\Users\\Wellbe\\DataspellProjects\\dsProject\\football_api\\files\\2024-11-22\\data_info_pl.csv')\n",
    "    document = loader.load()\n",
    "    char_text_split = RecursiveCharacterTextSplitter(chunk_size=100000, chunk_overlap=10)\n",
    "    docs = char_text_split.split_documents(document)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    template = \"\"\" \n",
    "    Você é um assistente especializado em futebol, capaz de responder perguntas de maneira precisa e informada.\n",
    "    É muito importante que voce responda de forma direta quando os dados estiverem no banco de dados.\n",
    "    Caso não haja a informação, voce pode dizer algo como \"Segundo minhas pesquisas\" e completar com a resposta. Mas sempre da forma mais direta possivel.\n",
    "    Voce nao pode incluir códigos, não ser que seja pedido. \n",
    "    Tente sempre memorizar as 3 respostas anteriores para guardar de contexto.\n",
    "    Use as informações nos arquivos csv que foram disponibilizados para voce e dos documentos disponíveis para formular suas respostas.\n",
    "    Lembrando que voce tem todas dados sobre o campeonato brasileiro e campeonato ingles(premier league) e os dados junto dos significados das siglas que voce tem a disposição estao dentro do arquivo {combined_info}  \n",
    "    \n",
    "    É muito importante que voce responda de forma direta quando os dados estiverem no banco de dados.\n",
    "    \n",
    "    Contexto:\n",
    "    - Banco de dados: contém dados atualizados sobre jogadores, times, estatísticas e competições.\n",
    "    - Documentos de apoio: incluem explicações detalhadas sobre regras, história, termos e estratégias de futebol.\n",
    "    \n",
    "    Arquivos com as estatisticas: {database_data}\n",
    "    Informações adicionais: {combined_info}\n",
    "    \n",
    "    Pergunta do usuário: {question}\n",
    "    \n",
    "    Resposta:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Preencher o prompt com os dados\n",
    "    prompt = PromptTemplate.from_template(template=template)\n",
    "    filled_prompt = prompt.format(database_data=docs, combined_info=combined_info, question=question)\n",
    "\n",
    "    # Conectar ao modelo de IA e obter a resposta\n",
    "    chat = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    response = chat.invoke(\n",
    "        input=[HumanMessage(content=filled_prompt)],\n",
    "        max_tokens=500  \n",
    "    )\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T00:02:05.650304Z",
     "start_time": "2024-12-17T00:02:05.639371Z"
    }
   },
   "id": "deaad6de001b3066",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01jb28t2ttetwa299x6x8c4hqa` on tokens per minute (TPM): Limit 30000, Requested 70014, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAPIStatusError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43manswer_question_with_glossary_and_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mQuem é o jogador com mais gols na premier league?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mcontent, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[5], line 54\u001B[0m, in \u001B[0;36manswer_question_with_glossary_and_data\u001B[1;34m(question)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# Conectar ao modelo de IA e obter a resposta\u001B[39;00m\n\u001B[0;32m     53\u001B[0m chat \u001B[38;5;241m=\u001B[39m ChatGroq(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama3-8b-8192\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 54\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mHumanMessage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilled_prompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m  \u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    285\u001B[0m         ChatGeneration,\n\u001B[1;32m--> 286\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    647\u001B[0m ]\n\u001B[0;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 633\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    637\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    638\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    639\u001B[0m         )\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:473\u001B[0m, in \u001B[0;36mChatGroq._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    468\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    469\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    472\u001B[0m }\n\u001B[1;32m--> 473\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:287\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    176\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    177\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m    Creates a model response for the given chat conversation.\u001B[39;00m\n\u001B[0;32m    179\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001B[39;00m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/openai/v1/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    299\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    300\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    303\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    305\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    306\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    307\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    308\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\groq\\_base_client.py:1244\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1230\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1231\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1232\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1239\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1240\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1241\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1242\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1243\u001B[0m     )\n\u001B[1;32m-> 1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\groq\\_base_client.py:936\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    928\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    929\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    934\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    935\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 936\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\groq\\_base_client.py:1039\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1036\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1038\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1039\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1042\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1043\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1047\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[0;32m   1048\u001B[0m )\n",
      "\u001B[1;31mAPIStatusError\u001B[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01jb28t2ttetwa299x6x8c4hqa` on tokens per minute (TPM): Limit 30000, Requested 70014, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "response = answer_question_with_glossary_and_data(\"Quem é o jogador com mais gols na premier league?\")\n",
    "print(response.content, end=\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T00:02:12.096158Z",
     "start_time": "2024-12-17T00:02:06.443355Z"
    }
   },
   "id": "1a6d1a14af7e5eab",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed34fd54a1f507dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def answer_question_api():\n",
    "    CHUNK_SIZE = 1000\n",
    "    OFFSET = 200\n",
    "\n",
    "    chromadb_path = \"path/to/save/\" # CONFIG YOUR PATH\n",
    "    chroma_client = chromadb.PersistentClient(path=chromadb_path)\n",
    "    collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "    def get_csv_content_as_string(file_path):\n",
    "        \"\"\"Read a csv file and return text in string\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    database_data = get_csv_content_as_string(r'C:\\Users\\Wellbe\\DataspellProjects\\dsProject\\football_api\\files\\2024-11-22\\data_info_pl.csv')\n",
    "    return database_data\n",
    "    database_data = get_csv_content_as_string()\n",
    "    \n",
    "    \n",
    "    def split_document(database_data=database_data):\n",
    "        \"\"\"Split a document in a list of string\"\"\"\n",
    "        \n",
    "        database_data_split = []\n",
    "        for i in range(0, len(database_data), CHUNK_SIZE):\n",
    "            start = i\n",
    "            end = i + 1000\n",
    "            if start != 0:\n",
    "                start = start - OFFSET\n",
    "                end =  end - OFFSET\n",
    "            database_data_split.append(database_data[start:end])\n",
    "            \n",
    "    return database_data_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5386e4cc4efb803",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19f22e4a0d9afa51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatOpenAI' from 'openai' (C:\\Users\\Wellbe\\miniconda3\\Lib\\site-packages\\openai\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'ChatOpenAI' from 'openai' (C:\\Users\\Wellbe\\miniconda3\\Lib\\site-packages\\openai\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T00:03:31.110948Z",
     "start_time": "2024-12-12T00:03:30.188755Z"
    }
   },
   "id": "bd5b9a40a20330ce",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "huggingfacehub_api_token = os.getenv(\"HUGGIN_FACE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T00:01:38.142387Z",
     "start_time": "2024-12-12T00:01:38.137269Z"
    }
   },
   "id": "407117d31742c0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mChatOpenAI\u001B[49m(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-4o-mini\u001B[39m\u001B[38;5;124m\"\u001B[39m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<OPENAI_API_TOKEN>\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Create a chat prompt template\u001B[39;00m\n\u001B[0;32m      4\u001B[0m prompt_template \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_messages(\n\u001B[0;32m      5\u001B[0m     [\n\u001B[0;32m      6\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are a helpful assistant.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      7\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRespond to question: \u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m     ]\n\u001B[0;32m      9\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key='<OPENAI_API_TOKEN>')\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "response = llm_chain.invoke({\"question\": \"How can I retain learning?\"})\n",
    "print(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T00:03:19.239471Z",
     "start_time": "2024-12-12T00:03:19.156976Z"
    }
   },
   "id": "6ba88cff6edf32fc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9085b1d77126476c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
